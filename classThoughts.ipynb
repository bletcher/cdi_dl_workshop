{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Day 1\n",
    "explain data inputs more clearly for each example on day 1 - array[row,col,rgb].  0-255\n",
    "As much as possible build up examples using the same datasets\n",
    "Day 2\n",
    "Helpful to show how the weights and bias influence sigmoid function. Reiterate that that's what W*B + b is doing \n",
    "dcnn explanation was confusing. I know what they are and I was a bit confused\n",
    "As going through dcnn, helpful to relate back to day 1 examples when talking about how conv layers are spatially distributed  \n",
    "I like to use https://cloud.google.com/vision/ to show what imageNet can do. I show very high flow and very low flow images from a stream  \n",
    "\n",
    "Your labelling approach comes out of nowhere\n",
    "Explain how your annotation app/approach fits into the larger image classification world. Annotations are usually a single label/image. Now we're annotating within images. Is this specific to landscapes.  \n",
    "Finally, at the end, you said, this is a preprocessing tool for labelling before the nn.\n",
    "\n",
    "Tried to make image bigger with python create_groundtruth\\label_1image_crf.py -w 200 -s 1.75  \n",
    "annotation lines were in wrong place on middle image of the output png\n",
    "\n",
    "In general, it would be useful to highlight key points, e.g. when retraining you get the advantage of the previously-trained nn, but you need to use the same architecture as the pretained nn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
